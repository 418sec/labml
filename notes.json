{
  "lab/logger/__init__.py": [
    {
      "pre": [
        "        _internal = _LoggerInternal()",
        "",
        "    return _internal",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def add_indicator(indicator: Indicator):",
        "    internal().add_indicator(indicator)",
        ""
      ],
      "code": [
        "def log(message: Union[str, List[Union[str, Tuple[str, StyleCode]]]],",
        "        color: List[StyleCode] or StyleCode or None = None,",
        "        *,",
        "        is_new_line=True):",
        "    if type(message) == str:",
        "        internal().log([(message, color)], is_new_line=is_new_line)",
        "    elif type(message) == list:",
        "        internal().log(message, is_new_line=is_new_line)"
      ],
      "note": "#### Print a message to the console",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "def store(*args, **kwargs):",
        "    internal().store(*args, **kwargs)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def new_line():",
        "    internal().new_line()",
        ""
      ],
      "code": [
        "def write():",
        "    internal().write()"
      ],
      "note": "#### Output the stored log values to screen, SQLite and TensorBoard",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [],
      "post": [
        "",
        "_internal: Optional[_LoggerInternal] = None",
        "",
        "",
        "def internal() -> _LoggerInternal:"
      ],
      "code": [
        "from typing import Union, List, Tuple, Optional, Iterable, Sized",
        "",
        "import numpy as np",
        "",
        "from .colors import StyleCode",
        "from .indicators import Indicator",
        "from .internal import LoggerInternal as _LoggerInternal"
      ],
      "note": "#### Imports",
      "collapsed": false,
      "codeCollapsed": true
    },
    {
      "pre": [
        "",
        "from .colors import StyleCode",
        "from .indicators import Indicator",
        "from .internal import LoggerInternal as _LoggerInternal",
        ""
      ],
      "post": [
        "",
        "",
        "def log(message: Union[str, List[Union[str, Tuple[str, StyleCode]]]],",
        "        color: List[StyleCode] or StyleCode or None = None,",
        "        *,"
      ],
      "code": [
        "_internal: Optional[_LoggerInternal] = None",
        "",
        "",
        "def internal() -> _LoggerInternal:",
        "    global _internal",
        "    if _internal is None:",
        "        _internal = _LoggerInternal()",
        "",
        "    return _internal"
      ],
      "note": "#### Internals\n\nInternals of the logger are hidden",
      "collapsed": true,
      "codeCollapsed": true
    },
    {
      "pre": [
        "        internal().log([(message, color)], is_new_line=is_new_line)",
        "    elif type(message) == list:",
        "        internal().log(message, is_new_line=is_new_line)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def store(*args, **kwargs):",
        "    internal().store(*args, **kwargs)",
        ""
      ],
      "code": [
        "def add_indicator(indicator: Indicator):",
        "    internal().add_indicator(indicator)"
      ],
      "note": "#### Setup a log indicator",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "def add_indicator(indicator: Indicator):",
        "    internal().add_indicator(indicator)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def write():",
        "    internal().write()",
        ""
      ],
      "code": [
        "def store(*args, **kwargs):",
        "    internal().store(*args, **kwargs)"
      ],
      "note": "#### Store values in the logger\n\nThese may be collected to a queue or a list depending on the log indicator configurations.",
      "collapsed": false,
      "codeCollapsed": false
    }
  ],
  "samples/mnist_loop.py": [
    {
      "pre": [
        "from lab.experiment.pytorch import Experiment",
        "from lab.logger.indicators import Queue, Histogram",
        "from lab.logger.util import pytorch as logger_util",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "class MNIST:",
        "    def __init__(self, c: 'Configs'):",
        "        self.model = c.model"
      ],
      "code": [
        "class Net(nn.Module):",
        "    def __init__(self):",
        "        super().__init__()",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)",
        "        self.fc1 = nn.Linear(4 * 4 * 50, 500)",
        "        self.fc2 = nn.Linear(500, 10)",
        "",
        "    def forward(self, x):",
        "        x = F.relu(self.conv1(x))",
        "        x = F.max_pool2d(x, 2, 2)",
        "        x = F.relu(self.conv2(x))",
        "        x = F.max_pool2d(x, 2, 2)",
        "        x = x.view(-1, 4 * 4 * 50)",
        "        x = F.relu(self.fc1(x))",
        "        x = self.fc2(x)",
        "        return F.log_softmax(x, dim=1)"
      ],
      "note": "#### Model",
      "collapsed": false,
      "codeCollapsed": true
    },
    {
      "pre": [
        "            self._train()",
        "            self._test()",
        "            self.__log_model_params()",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "class Configs(training_loop.TrainingLoopConfigs, LoaderConfigs):",
        "    epochs: int = 10",
        ""
      ],
      "code": [
        "class LoaderConfigs(configs.Configs):",
        "    train_loader: torch.utils.data.DataLoader",
        "    test_loader: torch.utils.data.DataLoader"
      ],
      "note": "#### Data loader configurations\n\nConfigs can be sub-classed",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "    set_seed = None",
        "",
        "    main: MNIST",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc()",
        "def device(*, use_cuda, cuda_device):",
        "    from lab.util.pytorch import get_device"
      ],
      "code": [
        "@Configs.calc()",
        "def data_loader_args(c: Configs):",
        "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}"
      ],
      "note": "`data_loader_args` configurations are computed based on other configs.\nThe name of the config is taken from the function name.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "                           transforms.Normalize((0.1307,), (0.3081,))",
        "                       ])),",
        "        batch_size=batch_size, shuffle=True, **dl_args)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc()",
        "def model(c: Configs):",
        "    m: Net = Net()"
      ],
      "code": [
        "@Configs.calc(['train_loader', 'test_loader'])",
        "def data_loaders(c: Configs):",
        "    train = _data_loader(True, c.batch_size, c.data_loader_args)",
        "    test = _data_loader(False, c.test_batch_size, c.data_loader_args)",
        "",
        "    return train, test"
      ],
      "note": "Multiple configs can be computed from a single function",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [],
      "post": [
        "",
        "",
        "class Net(nn.Module):",
        "    def __init__(self):",
        "        super().__init__()"
      ],
      "code": [
        "from typing import Dict",
        "",
        "import torch",
        "import torch.nn as nn",
        "import torch.nn.functional as F",
        "import torch.optim as optim",
        "import torch.utils.data",
        "from torchvision import datasets, transforms",
        "",
        "from lab import logger, configs",
        "from lab import training_loop",
        "from lab.experiment.pytorch import Experiment",
        "from lab.logger.indicators import Queue, Histogram",
        "from lab.logger.util import pytorch as logger_util"
      ],
      "note": "#### Imports",
      "collapsed": false,
      "codeCollapsed": true
    },
    {
      "pre": [
        "@Configs.calc()",
        "def data_loader_args(c: Configs):",
        "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "def _data_loader(is_train, batch_size, dl_args):",
        "    return torch.utils.data.DataLoader(",
        "        datasets.MNIST(str(logger.get_data_path()),"
      ],
      "code": [
        "@Configs.calc()",
        "def device(*, use_cuda, cuda_device):",
        "    from lab.util.pytorch import get_device",
        "",
        "    return get_device(use_cuda, cuda_device)"
      ],
      "note": "Here the function takes in a bunch of configs as **keyword only arguments*, instead of a single `Configs` object.\n\nAlthough the code looks cleaner this is not the advised way because it becomes harder to refactor later. It will be harder to find usages of a config for example.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "            output = self.model(data)",
        "            loss = F.nll_loss(output, target)",
        "            loss.backward()",
        "            self.optimizer.step()",
        ""
      ],
      "post": [
        "            logger.add_global_step()",
        "",
        "            if i % self.train_log_interval == 0:",
        "                logger.write()",
        ""
      ],
      "code": [
        "            logger.store(train_loss=loss)"
      ],
      "note": "This adds the training loss to the logger. It will queue them and output the mean.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "            logger.store(train_loss=loss)",
        "            logger.add_global_step()",
        "",
        "            if i % self.train_log_interval == 0:"
      ],
      "post": [
        "",
        "    def _test(self):",
        "        self.model.eval()",
        "        test_loss = 0",
        "        correct = 0"
      ],
      "code": [
        "                logger.write()"
      ],
      "note": "This writes the values stored in the logger to the screen and other destinations",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "class LoaderConfigs(configs.Configs):",
        "    train_loader: torch.utils.data.DataLoader",
        "    test_loader: torch.utils.data.DataLoader",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc()",
        "def data_loader_args(c: Configs):",
        "    return {'num_workers': 1, 'pin_memory': True} if c.device.type == 'cuda' else {}"
      ],
      "code": [
        "class Configs(training_loop.TrainingLoopConfigs, LoaderConfigs):",
        "    epochs: int = 10",
        "",
        "    loop_step = None",
        "    loop_count = None",
        "",
        "    is_save_models = True",
        "    batch_size: int = 64",
        "    test_batch_size: int = 1000",
        "",
        "    use_cuda: float = True",
        "    cuda_device: int = 0",
        "    seed: int = 5",
        "    train_log_interval: int = 10",
        "",
        "    is_log_parameters: bool = True",
        "",
        "    device: any",
        "",
        "    data_loader_args: Dict",
        "",
        "    model: nn.Module",
        "",
        "    learning_rate: float = 0.01",
        "    momentum: float = 0.5",
        "    optimizer: optim.SGD",
        "",
        "    set_seed = None",
        "",
        "    main: MNIST"
      ],
      "note": "### Configurations\n\nThis extends `TrainingLoopConfigs` and `LoaderConfigs`",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "    is_save_models = True",
        "    batch_size: int = 64",
        "    test_batch_size: int = 1000",
        ""
      ],
      "post": [
        "",
        "    is_log_parameters: bool = True",
        "",
        "    device: any",
        ""
      ],
      "code": [
        "    use_cuda: float = True",
        "    cuda_device: int = 0",
        "    seed: int = 5",
        "    train_log_interval: int = 10"
      ],
      "note": "Some configurations are set directly in declaration",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "",
        "def main():",
        "    conf = Configs()",
        "    experiment = Experiment(writers={'sqlite'})"
      ],
      "post": [
        "    experiment.add_models(dict(model=conf.model))",
        "    experiment.start()",
        "    conf.main()",
        "",
        ""
      ],
      "code": [
        "    experiment.calc_configs(conf,",
        "                            {'optimizer': 'adam_optimizer'},",
        "                            ['set_seed', 'main'])"
      ],
      "note": "We can provide a set of configs to be overridden in a dictionary.\n\nThe third parameter is a list of configs to be evaluated in order. It will try to evaluate `set_seed` as soon as possible and then evaluate `main` and it's dependencies.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "    m: Net = Net()",
        "    m.to(c.device)",
        "    return m",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc()",
        "def set_seed(c: Configs):",
        "    torch.manual_seed(c.seed)"
      ],
      "code": [
        "@Configs.calc('optimizer')",
        "def sgd_optimizer(c: Configs):",
        "    return optim.SGD(c.model.parameters(), lr=c.learning_rate, momentum=c.momentum)",
        "",
        "",
        "@Configs.calc('optimizer')",
        "def adam_optimizer(c: Configs):",
        "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)"
      ],
      "note": "Multiple options for configs can be provided.\nName of the option is inferred from the function name unless explicitly provided.",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "@Configs.calc('optimizer')",
        "def adam_optimizer(c: Configs):",
        "    return optim.Adam(c.model.parameters(), lr=c.learning_rate)",
        "",
        ""
      ],
      "post": [
        "",
        "",
        "@Configs.calc()",
        "def loop_count(c: Configs):",
        "    return c.epochs * len(c.train_loader)"
      ],
      "code": [
        "@Configs.calc()",
        "def set_seed(c: Configs):",
        "    torch.manual_seed(c.seed)"
      ],
      "note": "This is actually not a configuration. It sets the seed. Note that the function doesn't return any value.\n\nWhen we run the experiment we can inform the lab to run this as soon as possible. ",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "                output = self.model(data)",
        "                test_loss += F.nll_loss(output, target, reduction='sum').item()",
        "                pred = output.argmax(dim=1, keepdim=True)",
        "                correct += pred.eq(target.view_as(pred)).sum().item()",
        ""
      ],
      "post": [
        "",
        "    def __log_model_params(self):",
        "        if not self.__is_log_parameters:",
        "            return",
        ""
      ],
      "code": [
        "        logger.store(test_loss=test_loss / len(self.test_loader.dataset))",
        "        logger.store(accuracy=correct / len(self.test_loader.dataset))"
      ],
      "note": "Add test loss and accuracy to the logger",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "",
        "    def __log_model_params(self):",
        "        if not self.__is_log_parameters:",
        "            return",
        ""
      ],
      "post": [
        "",
        "    def __call__(self):",
        "        logger_util.add_model_indicators(self.model)",
        "",
        "        logger.add_indicator(Queue(\"train_loss\", 20, True))"
      ],
      "code": [
        "        logger_util.store_model_indicators(self.model)"
      ],
      "note": "Add histograms of model parameters and gradients",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "            return",
        "",
        "        logger_util.store_model_indicators(self.model)",
        "",
        "    def __call__(self):"
      ],
      "post": [
        "",
        "        logger.add_indicator(Queue(\"train_loss\", 20, True))",
        "        logger.add_indicator(Histogram(\"test_loss\", True))",
        "        logger.add_indicator(Histogram(\"accuracy\", True))",
        ""
      ],
      "code": [
        "        logger_util.add_model_indicators(self.model)"
      ],
      "note": "Setup logger indicators for model parameters and gradients",
      "collapsed": false,
      "codeCollapsed": false
    },
    {
      "pre": [
        "        logger_util.store_model_indicators(self.model)",
        "",
        "    def __call__(self):",
        "        logger_util.add_model_indicators(self.model)",
        ""
      ],
      "post": [
        "",
        "        for _ in self.loop:",
        "            self._train()",
        "            self._test()",
        "            self.__log_model_params()"
      ],
      "code": [
        "        logger.add_indicator(Queue(\"train_loss\", 20, True))",
        "        logger.add_indicator(Histogram(\"test_loss\", True))",
        "        logger.add_indicator(Histogram(\"accuracy\", True))"
      ],
      "note": "Setup log indicators",
      "collapsed": false,
      "codeCollapsed": false
    }
  ]
}